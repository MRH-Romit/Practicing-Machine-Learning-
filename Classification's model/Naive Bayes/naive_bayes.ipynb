{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53868e9a",
   "metadata": {},
   "source": [
    "# Naive Bayes Classification\n",
    "\n",
    "Naive Bayes is a probabilistic classification algorithm based on Bayes' theorem with the \"naive\" assumption of independence between features. The algorithm works by:\n",
    "\n",
    "1. **Bayes' Theorem**: P(A|B) = P(B|A) * P(A) / P(B)\n",
    "2. **Naive Assumption**: Features are independent of each other\n",
    "3. **Classification**: Assigns the class with the highest posterior probability\n",
    "\n",
    "In this notebook, we'll implement Naive Bayes for a classification task to predict whether a user will purchase a product based on their age and estimated salary using the Social Network Ads dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc167116",
   "metadata": {},
   "source": [
    "## Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15bf0699",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be48dc7",
   "metadata": {},
   "source": [
    "## Importing the dataset\n",
    "\n",
    "We're using the 'Social_Network_Ads.csv' dataset which contains information about users including their age, estimated salary, and whether they purchased a product (1) or not (0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49a010df",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('Social_Network_Ads.csv')\n",
    "X = dataset.iloc[:, :-1].values\n",
    "y = dataset.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2dbae2",
   "metadata": {},
   "source": [
    "## Exploring the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50a49d24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (400, 3)\n",
      "\n",
      "First 5 rows:\n",
      "   Age  EstimatedSalary  Purchased\n",
      "0   19            19000          0\n",
      "1   35            20000          0\n",
      "2   26            43000          0\n",
      "3   27            57000          0\n",
      "4   19            76000          0\n",
      "\n",
      "Dataset info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 400 entries, 0 to 399\n",
      "Data columns (total 3 columns):\n",
      " #   Column           Non-Null Count  Dtype\n",
      "---  ------           --------------  -----\n",
      " 0   Age              400 non-null    int64\n",
      " 1   EstimatedSalary  400 non-null    int64\n",
      " 2   Purchased        400 non-null    int64\n",
      "dtypes: int64(3)\n",
      "memory usage: 9.5 KB\n",
      "None\n",
      "\n",
      "Class distribution:\n",
      "Purchased\n",
      "0    257\n",
      "1    143\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Dataset shape:\", dataset.shape)\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(dataset.head())\n",
    "print(\"\\nDataset info:\")\n",
    "print(dataset.info())\n",
    "print(\"\\nClass distribution:\")\n",
    "print(dataset['Purchased'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123ece00",
   "metadata": {},
   "source": [
    "## Splitting the dataset into the Training set and Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61c94cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627f0805",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training set shape:\", X_train.shape)\n",
    "print(\"Test set shape:\", X_test.shape)\n",
    "print(\"\\nTraining set samples:\")\n",
    "print(X_train[:5])\n",
    "print(\"\\nTraining labels:\")\n",
    "print(y_train[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537c7796",
   "metadata": {},
   "source": [
    "## Feature Scaling\n",
    "\n",
    "While Naive Bayes doesn't require feature scaling for classification performance (since it works with probabilities), scaling can help with:\n",
    "1. Better visualization of decision boundaries\n",
    "2. Numerical stability in some implementations\n",
    "3. Consistency with other machine learning workflows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f48fc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcea7aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Scaled training set:\")\n",
    "print(X_train[:5])\n",
    "print(\"\\nScaled test set:\")\n",
    "print(X_test[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec41913",
   "metadata": {},
   "source": [
    "## Training the Naive Bayes model on the Training set\n",
    "\n",
    "We'll use Gaussian Naive Bayes which assumes that features follow a normal (Gaussian) distribution. This is appropriate for continuous features like age and salary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41018a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "classifier = GaussianNB()\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca2bd67",
   "metadata": {},
   "source": [
    "## Understanding Bayes' Theorem in our context\n",
    "\n",
    "Let's explore how Bayes' theorem applies to our classification problem:\n",
    "\n",
    "**P(Purchase|Age, Salary) = P(Age, Salary|Purchase) Ã— P(Purchase) / P(Age, Salary)**\n",
    "\n",
    "Where:\n",
    "- P(Purchase|Age, Salary): Probability of purchase given age and salary\n",
    "- P(Age, Salary|Purchase): Likelihood of age and salary given purchase decision\n",
    "- P(Purchase): Prior probability of purchase\n",
    "- P(Age, Salary): Evidence (normalizing constant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf53da0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get class priors (prior probabilities)\n",
    "class_priors = classifier.class_prior_\n",
    "print(\"Class priors (P(class)):\")\n",
    "print(f\"P(Not Purchase) = {class_priors[0]:.4f}\")\n",
    "print(f\"P(Purchase) = {class_priors[1]:.4f}\")\n",
    "\n",
    "# Get feature means for each class\n",
    "feature_means = classifier.theta_\n",
    "print(\"\\nFeature means for each class:\")\n",
    "print(f\"Not Purchase class - Age mean: {feature_means[0][0]:.4f}, Salary mean: {feature_means[0][1]:.4f}\")\n",
    "print(f\"Purchase class - Age mean: {feature_means[1][0]:.4f}, Salary mean: {feature_means[1][1]:.4f}\")\n",
    "\n",
    "# Get feature variances for each class\n",
    "feature_vars = classifier.var_\n",
    "print(\"\\nFeature variances for each class:\")\n",
    "print(f\"Not Purchase class - Age var: {feature_vars[0][0]:.4f}, Salary var: {feature_vars[0][1]:.4f}\")\n",
    "print(f\"Purchase class - Age var: {feature_vars[1][0]:.4f}, Salary var: {feature_vars[1][1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b046802",
   "metadata": {},
   "source": [
    "## Predicting a new result\n",
    "\n",
    "Let's predict whether a 30-year-old person with an $87,000 salary will purchase the product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f1da0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict for a new sample\n",
    "new_sample = [[30, 87000]]\n",
    "prediction = classifier.predict(sc.transform(new_sample))\n",
    "probability = classifier.predict_proba(sc.transform(new_sample))\n",
    "\n",
    "print(f\"Prediction for [Age: 30, Salary: $87,000]: {prediction[0]}\")\n",
    "print(f\"Probability of not purchasing: {probability[0][0]:.4f}\")\n",
    "print(f\"Probability of purchasing: {probability[0][1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d288ee",
   "metadata": {},
   "source": [
    "## Predicting the Test set results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0e7688",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(X_test)\n",
    "y_pred_proba = classifier.predict_proba(X_test)\n",
    "\n",
    "# Display predictions vs actual\n",
    "print(\"Predictions vs Actual (first 20 samples):\")\n",
    "comparison = np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1)\n",
    "print(\"Predicted | Actual\")\n",
    "print(\"----------|--------\")\n",
    "for i in range(20):\n",
    "    print(f\"    {comparison[i][0]}     |    {comparison[i][1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9629ac4",
   "metadata": {},
   "source": [
    "## Evaluating the Model Performance\n",
    "\n",
    "We'll evaluate our Naive Bayes model using multiple metrics:\n",
    "\n",
    "1. **Confusion Matrix** - Shows the counts of true positives, false positives, true negatives, and false negatives\n",
    "2. **Accuracy Score** - The proportion of correct predictions\n",
    "3. **Classification Report** - Includes precision, recall, F1-score, and support for each class\n",
    "4. **ROC Curve** - Shows the trade-off between true positive rate and false positive rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b89e11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, roc_curve, auc\n",
    "import seaborn as sns\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "# Visualize confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Not Purchase', 'Purchase'],\n",
    "            yticklabels=['Not Purchase', 'Purchase'])\n",
    "plt.title('Confusion Matrix - Naive Bayes')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()\n",
    "\n",
    "# Accuracy Score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"\\nAccuracy Score: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "\n",
    "# Classification Report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['Not Purchase', 'Purchase']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79852b2c",
   "metadata": {},
   "source": [
    "## ROC Curve and AUC Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1b86dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba[:, 1])\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random classifier')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve - Naive Bayes')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(f\"AUC Score: {roc_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d81f575",
   "metadata": {},
   "source": [
    "## Visualising the Training set results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f4a7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "# Visualize training set\n",
    "X_set, y_set = sc.inverse_transform(X_train), y_train\n",
    "X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 10, stop = X_set[:, 0].max() + 10, step = 0.5),\n",
    "                     np.arange(start = X_set[:, 1].min() - 1000, stop = X_set[:, 1].max() + 1000, step = 0.5))\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.contourf(X1, X2, classifier.predict(sc.transform(np.array([X1.ravel(), X2.ravel()]).T)).reshape(X1.shape),\n",
    "             alpha = 0.75, cmap = ListedColormap(['#FA8072', '#1E90FF']))\n",
    "plt.xlim(X1.min(), X1.max())\n",
    "plt.ylim(X2.min(), X2.max())\n",
    "\n",
    "for i, j in enumerate(np.unique(y_set)):\n",
    "    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1], \n",
    "                c = ListedColormap(['#FA8072', '#1E90FF'])(i), label = f'Class {j}', s=50, edgecolors='black')\n",
    "\n",
    "plt.title('Naive Bayes Classification (Training set)', fontsize=16)\n",
    "plt.xlabel('Age', fontsize=14)\n",
    "plt.ylabel('Estimated Salary', fontsize=14)\n",
    "plt.legend(['Not Purchase', 'Purchase'])\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd58540",
   "metadata": {},
   "source": [
    "## Visualising the Test set results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b589010e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize test set\n",
    "X_set, y_set = sc.inverse_transform(X_test), y_test\n",
    "X1, X2 = np.meshgrid(\n",
    "    np.arange(start=X_set[:, 0].min() - 1, stop=X_set[:, 0].max() + 1, step=0.5),\n",
    "    np.arange(start=X_set[:, 1].min() - 1, stop=X_set[:, 1].max() + 1, step=0.5)\n",
    ")\n",
    "\n",
    "# Predict for each point on the grid\n",
    "Z = classifier.predict(sc.transform(np.array([X1.ravel(), X2.ravel()]).T)).reshape(X1.shape)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "# Plot the decision boundary\n",
    "plt.contourf(X1, X2, Z, alpha=0.75, cmap = ListedColormap(['#FA8072', '#1E90FF']))\n",
    "plt.xlim(X1.min(), X1.max())\n",
    "plt.ylim(X2.min(), X2.max())\n",
    "\n",
    "# Define colors for scatter plot\n",
    "colors = ['#FA8072', '#1E90FF']\n",
    "# Plot the test set points\n",
    "for i, j in enumerate(np.unique(y_set)):\n",
    "    plt.scatter(\n",
    "        X_set[y_set == j, 0], X_set[y_set == j, 1],\n",
    "        color=colors[i], label=f'Class {j}', s=50, edgecolors='black'\n",
    "    )\n",
    "\n",
    "# Add titles and labels\n",
    "plt.title('Naive Bayes Classification (Test set)', fontsize=16)\n",
    "plt.xlabel('Age', fontsize=14)\n",
    "plt.ylabel('Estimated Salary', fontsize=14)\n",
    "plt.legend(['Not Purchase', 'Purchase'])\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d193ff",
   "metadata": {},
   "source": [
    "## Comparing with K-NN Performance\n",
    "\n",
    "Let's compare our Naive Bayes model with a K-NN model on the same dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5efb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Train K-NN model for comparison\n",
    "knn_classifier = KNeighborsClassifier(n_neighbors=5, metric='minkowski', p=2)\n",
    "knn_classifier.fit(X_train, y_train)\n",
    "y_pred_knn = knn_classifier.predict(X_test)\n",
    "\n",
    "# Compare performance\n",
    "nb_accuracy = accuracy_score(y_test, y_pred)\n",
    "knn_accuracy = accuracy_score(y_test, y_pred_knn)\n",
    "\n",
    "print(\"Model Comparison:\")\n",
    "print(f\"Naive Bayes Accuracy: {nb_accuracy:.4f} ({nb_accuracy*100:.2f}%)\")\n",
    "print(f\"K-NN Accuracy:       {knn_accuracy:.4f} ({knn_accuracy*100:.2f}%)\")\n",
    "print(f\"Difference:           {abs(nb_accuracy - knn_accuracy):.4f}\")\n",
    "\n",
    "# Cross-validation comparison\n",
    "nb_cv_scores = cross_val_score(classifier, X_train, y_train, cv=10)\n",
    "knn_cv_scores = cross_val_score(knn_classifier, X_train, y_train, cv=10)\n",
    "\n",
    "print(\"\\nCross-Validation Results (10-fold):\")\n",
    "print(f\"Naive Bayes CV Score: {nb_cv_scores.mean():.4f} (+/- {nb_cv_scores.std() * 2:.4f})\")\n",
    "print(f\"K-NN CV Score:        {knn_cv_scores.mean():.4f} (+/- {knn_cv_scores.std() * 2:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5deabe07",
   "metadata": {},
   "source": [
    "## Understanding the Naive Assumption\n",
    "\n",
    "The \"naive\" assumption in Naive Bayes is that features are independent. Let's examine this assumption with our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c9b4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check correlation between features\n",
    "original_data = pd.read_csv('Social_Network_Ads.csv')\n",
    "correlation = original_data[['Age', 'EstimatedSalary']].corr()\n",
    "\n",
    "print(\"Feature Correlation Matrix:\")\n",
    "print(correlation)\n",
    "\n",
    "# Visualize correlation\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(correlation, annot=True, cmap='coolwarm', center=0,\n",
    "            square=True, linewidths=0.5)\n",
    "plt.title('Feature Correlation Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Scatter plot to visualize relationship\n",
    "plt.figure(figsize=(10, 6))\n",
    "colors = ['red' if x == 0 else 'blue' for x in original_data['Purchased']]\n",
    "plt.scatter(original_data['Age'], original_data['EstimatedSalary'], c=colors, alpha=0.6)\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Estimated Salary')\n",
    "plt.title('Age vs Estimated Salary (Red: No Purchase, Blue: Purchase)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nCorrelation coefficient between Age and Salary: {correlation.iloc[0,1]:.4f}\")\n",
    "if abs(correlation.iloc[0,1]) < 0.3:\n",
    "    print(\"The features show weak correlation, supporting the naive assumption.\")\n",
    "elif abs(correlation.iloc[0,1]) < 0.7:\n",
    "    print(\"The features show moderate correlation, which may violate the naive assumption.\")\n",
    "else:\n",
    "    print(\"The features show strong correlation, violating the naive assumption.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e63e307",
   "metadata": {},
   "source": [
    "## Advantages and Disadvantages of Naive Bayes\n",
    "\n",
    "### Advantages:\n",
    "1. **Fast and Simple**: Quick to train and make predictions\n",
    "2. **Works well with small datasets**: Doesn't require large amounts of training data\n",
    "3. **Handles multiple classes naturally**: Can easily extend to multi-class classification\n",
    "4. **Not sensitive to irrelevant features**: The independence assumption helps ignore irrelevant features\n",
    "5. **Provides probability estimates**: Gives confidence levels for predictions\n",
    "\n",
    "### Disadvantages:\n",
    "1. **Strong independence assumption**: Rarely holds in real-world data\n",
    "2. **Categorical inputs need smoothing**: Zero probabilities can be problematic\n",
    "3. **Poor estimator for probability**: The independence assumption affects probability calibration\n",
    "4. **Requires prior assumptions**: Assumes feature distributions (Gaussian, Multinomial, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8221665a",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we implemented the Naive Bayes algorithm using Bayes' theorem to predict whether users would purchase a product based on their age and estimated salary. Key findings:\n",
    "\n",
    "1. **Model Performance**: The Naive Bayes classifier achieved good accuracy on the test set\n",
    "2. **Bayes' Theorem Application**: We saw how prior probabilities and likelihoods combine to make predictions\n",
    "3. **Feature Independence**: While the \"naive\" assumption may not perfectly hold, the model still performs well\n",
    "4. **Comparison with K-NN**: Both algorithms show competitive performance on this dataset\n",
    "5. **Probabilistic Nature**: Naive Bayes provides probability estimates, making it useful for confidence-based decisions\n",
    "\n",
    "The Naive Bayes algorithm is particularly useful when:\n",
    "- You have limited training data\n",
    "- You need fast predictions\n",
    "- You want probability estimates\n",
    "- Features are approximately independent\n",
    "\n",
    "This makes it an excellent choice for many real-world classification problems, especially in text classification, spam detection, and medical diagnosis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
